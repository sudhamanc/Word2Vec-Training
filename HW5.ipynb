{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3HEYAf_bjRJ6"
   },
   "source": [
    "# CS 614 - Applications of Machine Learning\r",
    "## Programming Assignment 5 - Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we'll play around with training a Word2Vec model\n",
    "\n",
    "We will generally follow the following article/tutorial:\n",
    "https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Our Data\n",
    "First let's create our data.   Create a few sentences that have similar words in them.  Here's some to get you started..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"What is the best time to call you tomorrow?\",  \"What is the best hour to call you tomorrow?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing Data\n",
    "Next let's extract the following information from the sentences\n",
    "1. Sentences\n",
    "2. How many times each word occurs\n",
    "3. And key index pair for each word\n",
    "4. A reverse index key\n",
    "\n",
    "We'll crudly *tokenize* our data by splitting using spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOZs2OKVa_BT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "## just a little bit of preprocessing for our data, counting/ordering types\n",
    "data = {'sentences': [s for x in sentences\n",
    "                      for s in [(x).lower().split()]]}  #the sentences\n",
    "\n",
    "for i in range(len(data['sentences'])):\n",
    "    data['sentences'][i] = [e for i in data['sentences'][i] for e in [i]][:-1]\n",
    "\n",
    "\n",
    "data['counts'] = Counter([t for s in data['sentences'] for t in s]) #unique words and their counts\n",
    "data['word2index'] = {t: i for i, t in enumerate(data['counts'])}  #indices for each word\n",
    "data['index2word'] = {v: k for k, v in data['word2index'].items()}\n",
    "\n",
    "print(data)\n",
    "print(len(data['word2index']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlU_zYdEUzab"
   },
   "source": [
    "### Setting up our Training Data\n",
    "To create our dataset we will loop through each sentence and:\n",
    "1. With a moving window of odd integer size *WINDOW_SIZE*, loop through the sentence.\n",
    "2. All middle words are the thing we want to predict ($y$)\n",
    "3. All other words in the window are our context, and will be $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "WINDOW_SIZE = 5  #TODO:  Play with this\n",
    "half = WINDOW_SIZE//2\n",
    "Xtrain = []\n",
    "Ytrain = []\n",
    "\n",
    "for s in data['sentences']:  #for each sentence\n",
    "    for i in range(0,len(s)-WINDOW_SIZE+1): #for each word in sentence\n",
    "        T = [data['word2index'][s[j]] for j in range(i,i+WINDOW_SIZE)]  #grab the word indices for the window\n",
    "        Ytrain.append([T[half]])\n",
    "        Xtrain.append(T[:half]+T[half+1:])\n",
    "\n",
    "Xtrain = torch.tensor(Xtrain, dtype=torch.long)\n",
    "Ytrain = torch.tensor(Ytrain, dtype=torch.long)\n",
    "\n",
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model\n",
    "Our model will be quite simple:\n",
    "1. Fully-connected layer to take us to our desired embedded dimension.\n",
    "2. Another fully-connected layer to take us to the number of classes (potential words)\n",
    "\n",
    "However, since we'll need to so some custom operations (summing over embedding in words of a sample), we'll create our own type of model (instead of just pre-made layers in a sequential model) so we can have more control over the forward (and backward) propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "VOCAB_SIZE = len(data['counts'])\n",
    "EMBED_DIMENSION = 300  #TODO:  Play with this\n",
    "\n",
    "class CBOW_Model(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dims):\n",
    "        super().__init__()\n",
    "        self.embedded_dim = embed_dims\n",
    "        \n",
    "        self.embeddings = nn.Linear(vocab_size, embed_dims)\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=embed_dims,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs_):\n",
    "        x = torch.zeros(inputs_.shape[0],inputs_.shape[1],self.embedded_dim)  #shape of output.  (samples x items in sample x embedded dims)\n",
    "        \n",
    "        for i in range(inputs_.shape[0]):  #for each training sample\n",
    "            x[i] = self.embeddings(inputs_[i])  #get its embeddings for the words in the sample\n",
    "        \n",
    "        x = x.mean(axis=1)  #take the average embedding\n",
    "        x = self.linear(x)   #predict\n",
    "        return x\n",
    "\n",
    "model = CBOW_Model(VOCAB_SIZE, EMBED_DIMENSION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "Now time for our world-famous training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 1000 #TODO:  Play with this\n",
    "\n",
    "#Choose your loss function and optimizer\n",
    "\n",
    "loss_fn = \n",
    "optimizer = \n",
    "\n",
    "model.train()\n",
    "running_loss = []\n",
    "\n",
    "XOneHot = torch.nn.functional.one_hot(Xtrain).to(torch.float)\n",
    "print(Xtrain)\n",
    "print(XOneHot)\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(XOneHot)\n",
    "   \n",
    "    loss = loss_fn(outputs, Ytrain.squeeze())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss.append(loss.item())\n",
    "\n",
    "    break\n",
    "#TODO:  Visualize the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Words\n",
    "Now for some fun.  Let's find similar words!\n",
    "\n",
    "To do this, we'll get the embeddings for all our words, make them unit length, the compare each by taking their dot product.  This is often known as the *cosine similarity*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(model.parameters())[0].detach().numpy().T   #Get the weights from the embedded layer\n",
    "print(weights.shape)\n",
    "ids = torch.nn.functional.one_hot(torch.arange(VOCAB_SIZE))  #The indices of all the words\n",
    "embeddings = ids@weights  # The embeddings of the words\n",
    "print(embeddings.shape)\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2) + 10**(-10)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms\n",
    "\n",
    "\n",
    "#This function takes a word (as a string), gets its embedding, compares to all other embeddings via cosine similarity.\n",
    "#Then returns the topN\n",
    "def get_top_similar(word: str, topN: int = 10):\n",
    "    try:\n",
    "        word_id = data['word2index'][word]   #This word's ID\n",
    "    except:\n",
    "        print(\"Out of vocabulary word\")\n",
    "        return\n",
    "\n",
    "    word_vec = embeddings_norm[word_id]   #This word's normalized embedding\n",
    "        \n",
    "    word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
    "    dists = np.matmul(embeddings_norm, word_vec).flatten()  #Dot product with all the other embeddings\n",
    "    topN_ids = np.argsort(-dists)[1 : topN + 1]  #Sort by most similar\n",
    "\n",
    "    topN_dict = {}\n",
    "    for sim_word_id in topN_ids:\n",
    "        sim_word = data['index2word'][sim_word_id.item()]\n",
    "        topN_dict[sim_word] = dists[sim_word_id]\n",
    "    return topN_dict\n",
    "\n",
    "myword = \"time\"\n",
    "try:\n",
    "    for word, sim in get_top_similar(myword).items():\n",
    "        print(\"{}: {:.3f}\".format(word, sim))\n",
    "except:\n",
    "    print(\"Word doesn't exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
